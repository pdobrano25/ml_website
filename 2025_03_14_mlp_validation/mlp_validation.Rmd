---
title: "MLP Validation"
output: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("~/Documents/PhD/ml_website/2025_03_14_mlp_validation")
library("dplyr")
```

## Microbial Load Predictor

The **Microbial Load Predictor** **(MLP)** is a tool validated and published by Nishijima et al in Cell, 2024
[Link to paper](10.1016/j.cell.2024.10.022)

It enables the prediction of bacterial cell counts in a metagenomic sample using the relative abundance values of individual taxa. Impressively, it works for 16S data.

Below, I sought to validate the analysis that enabled the MLP to work.

# **Validating** the analysis

## Load training data

First, we will need to download the training data from Nishijima's github directory.

The **training data** were originally obtained from [Vandeputte 2021](https://www.nature.com/articles/s41467-021-27098-7)

We will extract the training data from Nishijima's machine learning model.

Here are the first 10 rows of the training data, the first 4 features, and the measured microbial load ("outcome"):

```{r}

# load their XGBoost model (a caret object)
nishijima.model = readRDS("./model.16S_rRNA.rds")

# extract the training data
nishijima.training = nishijima.model$trainingData

# rename outcome
colnames(nishijima.training)[ncol(nishijima.training)] <- "outcome"

nishijima.training[1:10,c(1:4, ncol(nishijima.training))]

```

## Training data comments

If we glance at the training data, we see they log-transformed the data, which is an appropriate processing step for many machine learning models. Interestingly, it is not necessary for tree-based approaches, like XGBoost, which they used. 

Here is the distribution of *Faecalibacterium prausnitzii*, a highly abundant and prevalent gut microbe:

```{r}

# check normality
hist(nishijima.training[,1], 
     main=colnames(nishijima.training)[1],
     xlab = paste(colnames(nishijima.training)[1], "Log Abundance", sep=" "))

```

In order to log-transform, 0 values had to be replaced with a small *pseudocount*, which appears to be different for each taxa.

Here is a distribution of the minimum values of all taxa:

```{r}

# was the same value imputed for 0?
apply(nishijima.training, 1, min) %>% hist(xlab = "Minimum value", main="Pseudocount distribution")

```

This will be important to note later, when we discuss how data pre-processing impacts predictions.

## Load testing data

The **testing data** were obtained from [Vandeputte 2017](https://www.nature.com/articles/nature24460)

We can also download these data from Nishijima's github, however, they do not have the outcome data appended. So, let's pull the original source instead.

```{r, echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}

# load testing data (feature counts)
vandeputte.testing = read.csv("./Vandeputte_2017_16S.tsv", sep="\t") # supp table 14

# delete the sample name
vandeputte.testing$X = NULL

# make these count values into %
vandeputte.testing = sweep(vandeputte.testing, 1, rowSums(vandeputte.testing), FUN = "/")

# load microbial load measurements
vandeputte.testing.outcome = read.csv("./41586_2017_BFnature24460_MOESM14_ESM.csv")

# rename columns for easy access
colnames(vandeputte.testing.outcome) = c("Sample", "Health.status", "outcome", "Enterotype")

# lastly, log-scale outcome
vandeputte.testing.outcome$outcome = log(vandeputte.testing.outcome$outcome, base=10)

# do dimensions match?
nrow(vandeputte.testing.outcome) == nrow(vandeputte.testing)

```
```{r}
# here are the first 10x10 dimensions of the testing data
vandeputte.testing[1:10,c(1:4)]
```

## Test Nishijima's model

First, let's see how Nishijima's model performs "out-of-the-box".

We will compare the performance using **Internal Validation** (performed during model training and cross-validation) and, later, **External Validation** (performed on the testing dataset).

## Internal Validation

Here, we will extract the best model from Nishijima's saved model object:

```{r}

# extract Internal Validation predictions - from caret's "final model"
nishijima.model.predictions = nishijima.model$pred %>%
  # to extract the final model, we use the best hyperparameter values
  subset(eta == nishijima.model$bestTune$eta &
           nrounds == nishijima.model$bestTune$nrounds &
           max_depth == nishijima.model$bestTune$max_depth &
           gamma == nishijima.model$bestTune$gamma &
           colsample_bytree == nishijima.model$bestTune$colsample_bytree &
           min_child_weight == nishijima.model$bestTune$min_child_weight &
           subsample == nishijima.model$bestTune$subsample) %>%
  # across all cross-validation folds, we'll average the prediction values
  group_by(rowIndex) %>%
  mutate(ave.pred = mean(pred)) %>%
  dplyr::select(ave.pred, rowIndex, obs) %>% distinct() %>% data.frame()



```

Nishijima uses Pearson correlation coefficients to evaluate the predictive performance of their model. This should be appropriate because we're working on a regression problem to predict a continuous variable. The outcome is log-transformed microbial load, which is approximately normally distributed.

```{r}
hist(nishijima.model.predictions$obs, main="Microbial Load Distrubtion", xlab="Measured Microbial Load")
```

Let's see what the Pearson correlation coefficient is between the true (measured) values and the predicted (model output) values:

```{r}

# Here's a scatter plot with embedded correlation

ggpubr::ggscatter(nishijima.model.predictions, 
          x = "obs", y = "ave.pred",
          add = "reg.line", 
          conf.int = TRUE, 
          add.params = list(color = "blue",
                            fill = "lightgray"))+
  ggpubr::stat_cor(method = "pearson")+
  ggplot2::labs(x="Measured Microbial Load (log10)",
       y="Predicted Microbial Load (log10)",
       title="Internal Validation\nOriginal Model")

```

Good, this appears to recapitulate Nishijima's work.

## External Validation

Now, let's check the model's performance on the testing data.

```{r, echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}

# do features in model and testing data overlap?

colnames(nishijima.model$trainingData) %in% colnames(vandeputte.testing)
# many are missing
colnames(vandeputte.testing) %in% colnames(nishijima.model$trainingData)

# So, we need to add placeholder columns, which we assume to be 0 since they weren't detected 
# Let's make a function to make this easier later

process.for.model = function(model = model,
         testing = testing.data){
  
    # add missing variables to testing data
    missing.taxa = colnames(model$trainingData)[!colnames(model$trainingData) %in%
                                                                    colnames(testing)]
    if(length(missing.taxa)>0){
    testing = cbind(testing, 
                            do.call(cbind, lapply(missing.taxa, function(x){
                              new.data = data.frame(x = rep(0, times = nrow(testing)))
                              colnames(new.data) = x
                              new.data
                            }))) %>% data.frame()
    }
    # and, reorder to be the same as the training data
    testing = testing[,colnames(model$trainingData)]
    
    return(testing)
}

# Let's apply this function
vandeputte.testing.processed = process.for.model(nishijima.model, vandeputte.testing)

# Did it work?
colnames(nishijima.model$trainingData) %in% colnames(vandeputte.testing.processed)
# Yes

# We also need to add a pseudocount
pseudo.test.van.loop = min(vandeputte.testing.processed[vandeputte.testing.processed!=0])/2
# And log-transform
vandeputte.testing.processed = log(vandeputte.testing.processed+pseudo.test.van.loop, base=10)
    
# Apply model to predict microbial load of testing data
testing.predictions = predict(nishijima.model, vandeputte.testing.processed)

# Construct dataframe for Pearson correlation
testing.predictions = data.frame(pred = testing.predictions,
                                 obs = vandeputte.testing.outcome$outcome)
```
```{r}
# Lastly, check Pearson correlation
ggpubr::ggscatter(testing.predictions, 
          x = "obs", y = "pred",
          add = "reg.line", 
          conf.int = TRUE, 
          add.params = list(color = "blue",
                            fill = "lightgray"))+
  ggpubr::stat_cor(method = "pearson")+
  ggplot2::labs(x="Measured Microbial Load (log10)",
       y="Predicted Microbial Load (log10)",
       title="External Validation\nOriginal Model")
```

In Nishijima's publication, the external validation correlation was 0.60.

I suspect this is because of small nuances in the training data, particularly the pseudocount.

Let's rebuild their training data from the original source and process it in a way that is more amenable to external validation.

## Rebuild training data

```{r, echo=T, results=FALSE, warning=FALSE, message=FALSE}

vandeputte.training.rebuilt = read.csv("./2025_03_05_vandeputte_otu_unrarefied.csv")
vandeputte.load.rebuilt = read.csv("./2025_03_05_vandeputte_load.csv")[,c("X", "Cell_count_per_gram", "ID_Number", "Day_Number")]

# Note how many unique subjects there are:
unique(vandeputte.load.rebuilt$ID_Number) %>% length()
# n = 20 

# remove NA values, which can't be used for prediction
vandeputte.training.rebuilt = vandeputte.training.rebuilt[!is.na(vandeputte.load.rebuilt$Cell_count_per_gram),]

vandeputte.load.rebuilt = vandeputte.load.rebuilt[!is.na(vandeputte.load.rebuilt$Cell_count_per_gram),]

# Do feature values sum to 1?
rowSums(vandeputte.training.rebuilt)
# No, therefore we need to rarefy

# First, delete the sample name column
vandeputte.training.rebuilt$X = NULL

# Second, rarefy
set.seed(25)
vandeputte.training.rebuilt = phyloseq::rarefy_even_depth(phyloseq::otu_table(vandeputte.training.rebuilt, taxa_are_rows=F), 
                                     sample.size=10000,
                                     replace=F) %>% data.frame()
# Third, convert to %
vandeputte.training.rebuilt <- sweep(vandeputte.training.rebuilt, 1, rowSums(vandeputte.training.rebuilt), FUN = "/")

# Fourth, log-transform and save pseudocount
van.pseudo.keep = min(vandeputte.training.rebuilt[vandeputte.training.rebuilt!=0])/2
vandeputte.training.rebuilt = log(vandeputte.training.rebuilt+van.pseudo.keep, base=10)

# Last, append outcome, matching by rownames
vandeputte.training.rebuilt$outcome = log(vandeputte.load.rebuilt[rownames(vandeputte.training.rebuilt),]$Cell_count_per_gram, base=10)
```
```{r}
vandeputte.training.rebuilt[1:10,c(1:4, ncol(vandeputte.training.rebuilt))]
```

We've rebuilt the training data, now we can re-train the predictor.

## Re-train model

```{r, echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}

# Use their training grid
tune_grid = expand.grid(
  nrounds=c(100, 500, 1000),
  max_depth = c(3:5),
  eta = c(0.01, 0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = 1,
  min_child_weight = 1
)

# set seed for reproducibility
set.seed(25)

# train model with caret
t1 = Sys.time()
nishijima.model.rebuilt = caret::train(
  outcome ~ .,
  data = vandeputte.training.rebuilt,
  trControl = caret::trainControl(method="repeatedcv", 
                                  number=10, 
                                  repeats = 1, 
                                  savePredictions = TRUE),
  tuneGrid = tune_grid,
  method = "xgbTree",
  preProc = c("center", "scale")
)
t2 = Sys.time()
print(t2 - t1, digits=3)

nishijima.model.rebuilt.predictions = nishijima.model.rebuilt$pred %>%
  subset(eta == nishijima.model.rebuilt$bestTune$eta &
           nrounds == nishijima.model.rebuilt$bestTune$nrounds &
           max_depth == nishijima.model.rebuilt$bestTune$max_depth &
           gamma == nishijima.model.rebuilt$bestTune$gamma &
           colsample_bytree == nishijima.model.rebuilt$bestTune$colsample_bytree &
           min_child_weight == nishijima.model.rebuilt$bestTune$min_child_weight &
           subsample == nishijima.model.rebuilt$bestTune$subsample) %>%
  group_by(rowIndex) %>%
  mutate(ave.pred = mean(pred)) %>%
  dplyr::select(ave.pred, rowIndex, obs) %>% distinct() %>% data.frame()
```

```{r}

# Here's a scatter plot with embedded correlation
ggpubr::ggscatter(nishijima.model.rebuilt.predictions, 
          x = "obs", y = "ave.pred",
          add = "reg.line", 
          conf.int = TRUE, 
          add.params = list(color = "blue",
                            fill = "lightgray"))+
  ggpubr::stat_cor(method = "pearson")+
  ggplot2::labs(x="Measured Microbial Load (log10)",
       y="Predicted Microbial Load (log10)",
       title="Internal Validation\nRebuilt Model")
```

Our rebuilt model is comparable to the original model in terms of internal validation performance.

How does it compare on the external validation set?

```{r, echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}

# Reprocess testing data to have proper features
vandeputte.testing.rebuilt.processed = process.for.model(nishijima.model.rebuilt, vandeputte.testing)

# We also need to add a pseudocount (the same as the training data!)
# training pseudo
van.pseudo.keep
# testing pseudo
new.pseudo = min(vandeputte.testing.rebuilt.processed[vandeputte.testing.rebuilt.processed!=0])/2
# pick smaller pseudo
smaller.pseudo = min(van.pseudo.keep, new.pseudo)
# add pseudo and log-transform
vandeputte.testing.rebuilt.processed = log(vandeputte.testing.rebuilt.processed+smaller.pseudo, base=10)

# Apply rebuilt model to predict microbial load of testing data
testing.predictions.rebuilt = predict(nishijima.model.rebuilt, vandeputte.testing.rebuilt.processed)

# Construct dataframe for Pearson correlation
testing.predictions.rebuilt = data.frame(pred = testing.predictions.rebuilt,
                                 obs = vandeputte.testing.outcome$outcome)
```

```{r}
# Lastly, check Pearson correlation
ggpubr::ggscatter(testing.predictions.rebuilt, 
          x = "obs", y = "pred",
          add = "reg.line", 
          conf.int = TRUE, 
          add.params = list(color = "blue",
                            fill = "lightgray"))+
  ggpubr::stat_cor(method = "pearson")+
  ggplot2::labs(x="Measured Microbial Load (log10)",
       y="Predicted Microbial Load (log10)",
       title="External Validation\nOriginal Model")

```

This correlation (R = 0.64) is now comparable to the original correlation (R = 0.60).

Therefore, using non-standard pre-processing had a significant impact on the external validation performance.

# **Extending** the analysis

Now that we've validated the original results, we can ask additional questions.

With machine learning, a fair question is: *would other algorithms work better?*

## Other algorithms

Let's re-run this pipeline using different machine learning models.

We'll use common models included in caret:
linear models like **elastic net**, **support vector machines**, tree-based models like **random forest**, **gradient boosting machine**, and the original: **XGBoost**.

We're also going to reduce the number of CV from 10 to 5, to save on time, but repeat each model evaluation 15 times to see how variable the CV process is.

Let's see how long these model took to build:

```{r, echo=T, results=FALSE, warning=FALSE, message=FALSE}
# select models
ml.models = c("ranger", "gbm", "xgbTree", "lm", "glmnet", "svmLinear", "svmRadial")
```

```{r, echo=F, results=FALSE, warning=FALSE, message=FALSE, cache=TRUE}

t0 <- Sys.time()
model.loop = do.call(rbind, lapply(1:15, function(iter) {
  do.call(rbind, lapply(ml.models, function(model){
    print(paste0(iter, model))
    # train model with caret
    t1 = Sys.time()
    set.seed(iter)
    model.ml = caret::train(
      outcome ~ .,
      data = vandeputte.training.rebuilt,
      trControl = caret::trainControl(method="cv", 
                                      number=5, 
                                      savePredictions = TRUE),
      method = model,
      verbose=F,
      preProc = c("center", "scale")) %>% suppressWarnings()
    t2 = Sys.time()
    t2 - t1
    
    # record best model R2
    min.rmse = min(model.ml$results["RMSE"])
    
# Reprocess testing data to have proper features
vandeputte.testing.rebuilt.processed = process.for.model(nishijima.model.rebuilt, vandeputte.testing)

# We also need to add a pseudocount (use the smaller of the two datasets
# training pseudo
van.pseudo.keep
# testing pseudo
new.pseudo = min(vandeputte.testing.rebuilt.processed[vandeputte.testing.rebuilt.processed!=0])/2
# pick smaller pseudo
smaller.pseudo = min(van.pseudo.keep, new.pseudo)
# add pseudo and log-transform
vandeputte.testing.rebuilt.processed = log(vandeputte.testing.rebuilt.processed+smaller.pseudo, base=10)

    model.output = predict(model.ml, vandeputte.testing.rebuilt.processed)

    # save
    data.frame(true = vandeputte.testing.outcome$outcome,
               pred = model.output,
               iter = iter,
               model = model,
               time = as.numeric(t2 - t1),
               internal.rmse = min.rmse)
}))}))
t3 <- Sys.time()
t3 - t0
```


```{r, echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}


model.loop.pearson = do.call(rbind, lapply(1:15, function(seed) {
  do.call(rbind, lapply(ml.models, function(ml){
    print(paste0(ml, seed))
    data.subset = subset(model.loop, iter == seed & model == ml) 
    #
    spear = cor.test(data.subset$true,
             data.subset$pred, method="spearman")$estimate
    pear = cor.test(data.subset$true,
                     data.subset$pred, method="pearson")$estimate
    if(sd(data.subset$pred)==0){
      r2 = NA
    }else{
    r2 = summary(lm(scale(pred) ~ scale(true), data.subset))$r.squared
    }
    # save
    data.frame(model = ml,
               iter = seed,
               spearman = spear,
               pearson = pear,
               r2 = r2)
  }))}))
```

```{r}
# check how long each model takes to build
ggplot(model.loop[,c("iter", "model", "time")] %>% distinct())+
  geom_boxplot(aes(x = model, y=time))+
  geom_point(aes(x =  model, y=time), size=2.5, color="black")+
  geom_point(aes(x =  model, y=time), size=1.5, color="white")+
  geom_point(aes(x =  model, y=time, color= model), size=1.5, alpha=0.6)+
  theme_bw()+theme(legend.position="none")+
  labs(x="Model", y="Time elapsed (sec)")
# RandomForest is fast
```

This took about 45 minutes to run. 

Next, lets check the internal validation *RMSE*, the loss function caret uses to select the optimal hyperparameters.

```{r}
ggplot(model.loop[,c("iter", "model", "internal.rmse")] %>% distinct())+
  geom_boxplot(aes(x = model, y=internal.rmse))+
  geom_point(aes(x =  model, y=internal.rmse), size=2.5, color="black")+
  geom_point(aes(x =  model, y=internal.rmse), size=1.5, color="white")+
  geom_point(aes(x =  model, y=internal.rmse, color= model), size=1.5, alpha=0.6)+
  theme_bw()+theme(legend.position="none")+
  labs(x="Model", y="Internal Validation\nRMSE")
```

From the internal validation data (based on how caret selects optimal models), it appears like gradient boosting machine and random forest are slightly superior to XGBoost because they have lower RMSE values.

Now we will evaluate the performance by calculating the Pearson correlation coefficients for each model and each iteration.

```{r}
# plot pearson values
model.loop.pearson.plot = ggplot(model.loop.pearson)+
  geom_boxplot(aes(x = model, y=pearson))+
  geom_point(aes(x = model, y=pearson), size=2.5, color="black")+
  geom_point(aes(x = model, y=pearson), size=1.5, color="white")+
  geom_point(aes(x = model, y=pearson, color=model), size=1.5, alpha=0.6)+
  theme_bw()+theme(legend.position="none")+
  geom_hline(yintercept = 0, linetype=2)+
  labs(x="Model", y="Pearson Correlation")

model.loop.pearson.plot

```

From the above, we can conclude that XGBoost (xgbTree) performs well, but variably, on the validation data set.

Other models perform comparably, like elastic net (glmnet) and random forest (ranger). 

Importantly, we see that gradient boosting machine (gbm) has a slightly superior performance.


